{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe29c51",
   "metadata": {},
   "source": [
    "# Paso 1: Instalar las librerías y dependencias necesarias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df6ed945",
   "metadata": {},
   "source": [
    "###### El paso inicial, es naturalmente importar todas las librerías y dependencias que estas contienen\n",
    "###### PyCaret instalará automáticamente Pandas, Numpy, Scikit-learn y Pickle por nosotros como sus dependencias, por lo que no deberemos hacerlo nosotros mismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aefe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installar las librerías necesarias, se debe poner el signo de ! antes del comando, ya que es un comando de shell\n",
    "#!pip install datasets\n",
    "#!pip install pycaret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11f25c72",
   "metadata": {},
   "source": [
    "# Paso 2: Descargamos el dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "321c092d",
   "metadata": {},
   "source": [
    "###### El segundo paso como está indicado, es descargar el dataset que se encuentra en la página de HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las dependencias a usar\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45def9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos dentro de una variable el nombre del dataset a descargar \n",
    "dataset = load_dataset('hate_speech_offensive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos nuestro dataset de entrenamiento en un dataframe de Pandas\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos dentro de un csv nuestro archivo\n",
    "df.to_csv('hate_speech_offensive.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08e0915f",
   "metadata": {},
   "source": [
    "# Paso 3: Analizar la estructura de nuestro dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b326c6c4",
   "metadata": {},
   "source": [
    "###### Un paso muy importante antes de continuar, es analizar cuantos casos tiene por categoría nuestro dataset, de esta forma podremos ver si se trata de un dataset balanceado o no.\n",
    "###### ¿Es esto importante?, ¿por qué?\n",
    "###### Bueno cuando estamos trabajando con el entrenamiento de modelos, una de las cosas más importantes es verificar que no existe una sobre representación de ninguna de las categorías, de lo contrario, no tendremos un modelo eficiente, tenderá a tener un mayor nivel de error al predecir categorías si trabajamos con un marco de entrenamiento con sesgos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos nuestro dataset y lo guardamos en una variable/dataframe llamada df \n",
    "df = pd.read_csv('hate_speech_offensive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38524341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mediante pandas contamos la cantidad de tweets que corresponden a cada categoría dentro de la columna class\n",
    "class_counts = df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto \"imprime\" en consola la cantidad de tweets que corresponden a cada categoría\n",
    "print(class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "349e31df",
   "metadata": {},
   "source": [
    "# Paso 4: Crear un dataset balanceado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b78ce50d",
   "metadata": {},
   "source": [
    "###### Una vez que analizamos nuestro dataset, nos damos cuenta de que el mismo está desbalanceado, lo que debemos hacer entonces es aplicar una técnica mediante un método de Pandas, que es el \"muestreo\"(sample), para seleccionar solo la cantidad de casos por categoría que nosotros estimemos necesario y le indicaremos que seleccione al azar.\n",
    "###### Son tres categorías posibles de etiquetas en nuestro dataset, 2: para tweets normales, 1: para tweets de odio(racismo y sexismo) y 0: para tweets con lenguaje ofensivo(insultos generales).\n",
    "###### Le indicaremos que seleccione todos los tweets con lenguaje ofensivo(etiqueta 0) y de las otras dos categorías que solo seleccione 3mil, ya que consideramos que con eso tendremos un dataset balanceado y listo para entrenar, evitando sesgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d973444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el dataset en formato csv a usar y lo guardamos dentro de una variable/dataframe llamada \"df\"\n",
    "df = pd.read_csv('hate_speech_offensive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos tres dataframes independientes, cada uno conteniendo la cantidad de tweets que le indicamos y por etiqueta, \n",
    "# y creando una muestra al azar.\n",
    "df_0 = df[df['class'] == 0]\n",
    "df_1 = df[df['class'] == 1].sample(n=3000, random_state=1)\n",
    "df_2 = df[df['class'] == 2].sample(n=3000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d972bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenamos los tres dataframes anteriores para crear uno solo\n",
    "balanced_df = pd.concat([df_0, df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el nuevo dataset balanceado dentro de un nuevo archivo csv, conservando solo las columnas \"class\" y \"tweet\"\n",
    "#Ya que no necesitamos al resto de las columnas para nuestro modelo\n",
    "balanced_df[['class', 'tweet']].to_csv('fixed_dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f184e406",
   "metadata": {},
   "source": [
    "# Paso 5: Limpiar los tweets de caracteres molestos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7d216e1",
   "metadata": {},
   "source": [
    "###### Un paso muy importante, es limpiar los tweets de caracteres que podrían llegar a perjudicar la capacidad analítica de nuestro modelo, esto son los arrobas, enlaces http y otros elementos que no aportan nada a nuestro modelo predictivo, además de convertir todas las palabras a minúsculas para su mejor manejo. Para todo esto, usaremos \"expresiones regulares\" y usaremos una función lambda(callback) para lograrlo en cada etapa de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las dependencias\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27cfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el archivo csv generado en el paso anterior que fue el scraping de tweets\n",
    "df = pd.read_csv('fixed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos todos los datos de \"tweet\" a tipo string para no tener problemas de \"tipado\"\n",
    "df['tweet'] = df['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21606870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos expresiones regulares para eliminar caracteres indeseados, enlaces y arrobamiento, \n",
    "#y luego convertimos todo a minúsculas \n",
    "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazamos los espacios en blanco de la columna Comentario con \"Nan\"\n",
    "df['tweet'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos todas las filas que tengan valor \"Nan\" en la celda de la columna \"tweet\"\n",
    "df.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06179376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos todos los tweets duplicados y solo nos quedamos con uno de ellos en caso de existir más de uno\n",
    "df.drop_duplicates(subset='tweet', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336038a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquí seleccionamos solo las columnas \"class\" y \"tweet\" para guardar en el nuevo archivo csv\n",
    "df = df[['class', 'tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el resultado de la limpieza sin un índice\n",
    "df.to_csv('hate_speech_offensive_cleaned.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8c6dac1",
   "metadata": {},
   "source": [
    "# Paso 6: Entrenamos el modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67347559",
   "metadata": {},
   "source": [
    "###### Luego de que tenemos ya todos los tweets limpios y un dataset balanceado, es momento de la parte más importante de todas, entrenar un modelo de clasificación de tweets.\n",
    "###### Para ello dividiremos nuestro dataset en dos partes, elegidas cada una de ellas al azar(esto es importantísimo), un 15% será destinada a probar el modelo y el 85% restante está destinado a entrenar dicho modelo.\n",
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerías y dependencias\n",
    "from pycaret.classification import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dccdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar los datos que vayamos a usar\n",
    "df = pd.read_csv('hate_speech_offensive_cleaned.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf98f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nos aseguramos de que las columnas tengan el nombre correcto para que PyCaret haga un trabajo adecuado, por ello le cambiaremos\n",
    "#el nombre a las columnas \"class\" pasará a llamarse \"Label\" y \"tweet\" pasará a llamarse \"Text\"\n",
    "df.columns = ['Label', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos los textos en características utilizando TF-IDF. Este proceso asigna un puntaje a cada palabra en un tweet\n",
    "#dependiendo de cuántas veces aparece en el tweet y cuántas veces aparece en el resto del conjunto de datos. \n",
    "#Esto nos ayuda a destacar las palabras que son particularmente importantes en un tweet.\n",
    "#Usaremos la opción de quitar las palabras de parada ('stop-words'), ya que estas palabras (como 'a', 'and', 'the' en inglés) \n",
    "#aparecen tan frecuentemente que no aportan mucha información al modelo.\n",
    "#El parámetro max_features limita el número de palabras que consideraremos. En este caso, solo estamos tomando \n",
    "#las 1000 palabras más frecuentes.\n",
    "#El resultado de este proceso es una matriz donde cada fila representa un tweet y cada columna representa una de las \n",
    "#1000 palabras más frecuentes. El valor en una celda específica es el puntaje TF-IDF de una palabra para un tweet en particular.\n",
    "#En resumen, esta matriz nos ayuda a entender qué palabras son especialmente características para cada tweet y\n",
    "#nos permite predecir a qué categoría pertenece un tweet basándonos en estas palabras.\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "features = vectorizer.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c591dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un nuevo DataFrame con las características TF-IDF y las guardamos dentro de arrays\n",
    "df_tfidf = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf['Label'] = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38200974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir los datos en conjunto de entrenamiento y prueba\n",
    "train_df, test_df = train_test_split(df_tfidf, test_size=0.15, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf76eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el vectorizador para usarlo en la etapa de predicción\n",
    "pickle.dump(vectorizer, open(\"tfidf_vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d65f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuramos el módulo de clasificación de PyCaret\n",
    "clf = setup(data = train_df, target = 'Label', session_id = 21, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7529420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparamos los modelos para ver cuál tiene mayor capacidad de predicción\n",
    "best = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasamos a seleccionar el modelo que mejor resultado tuvo y lo finalizamos\n",
    "final_model = finalize_model(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el modelo que en la carpeta de destino\n",
    "save_model(final_model, model_name = \"hate_tweets_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94353bb6",
   "metadata": {},
   "source": [
    "# Paso 7: Probamos nuestro modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f20bdc4",
   "metadata": {},
   "source": [
    "###### El último paso de todos es probar si nuestro modelo funciona con una serie de \"tweets\" que crearemos para ver que tan efectivo resultó el entrenamiento realizado y si puede predecir efectivamente a la categoría que pertenece cada uno.\n",
    "###### Esto es sumamente importante antes de poner al modelo en \"producción\" (usar el modelo para nuestro propósito), ya que sino, no sabremos si funciona adecuadamente y pondría en riesgo nuestro trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc628b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar las librerías y dependencias\n",
    "from pycaret.classification import load_model, predict_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2009e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el modelo\n",
    "modelo = load_model('hate_tweets_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el vectorizador\n",
    "vectorizer = pickle.load(open(\"tfidf_vectorizer.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d28fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un nuevo tweet para probar si el modelo funciona\n",
    "nuevo_tweet = pd.DataFrame({\"Text\": [\"You are such a stupid bitch, damned fool\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos el tweet de prueba al mismo formato que los datos de entrenamiento\n",
    "tweet_transformado = vectorizer.transform(nuevo_tweet['Text'])\n",
    "df_tfidf = pd.DataFrame(tweet_transformado.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos la categoría a la que pertenece el tweet de prueba\n",
    "prediccion = predict_model(modelo, data=df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1afa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimimos todo el DataFrame de predicciones, para ver si ha hecho una buena predicción de nuestro nuevo tweet\n",
    "print(prediccion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
